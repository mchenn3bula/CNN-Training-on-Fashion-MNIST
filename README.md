# CNN Training on Fashion MNIST

This project explores the impact of Batch Normalization, optimizers, learning rates, and batch sizes on CNN performance using the Fashion MNIST dataset.

## Methods
- CNN with Conv2D, AveragePooling2D, Dense layers
- Batch Normalization & Standard Normalization
- Optimizers: SGD and Adam
- Learning Rate Finder
- Batch Size Experimentation

## Results & Observations
- Larger batch sizes reduce steps per epoch but don't guarantee lower loss.
- Batch Normalization with SGD behaves differently compared to Adam optimizer.

## Visualizations
![image_1.png](./image_1.png)

![image_2.png](./image_2.png)

![image_3.png](./image_3.png)

![image_4.png](./image_4.png)

![image_5.png](./image_5.png)

